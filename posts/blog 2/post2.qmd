---
title: "Clustering"
author: "Aniket Adhikari"
date: "2023-11-15"
date-modified: "2023-11-15"
toc: true
toc-title: "On this page"

---
# Supervised Learning vs. Unsupervised Learning
Before we talk about clustering, we must address some integral concepts related to it: supervised and unsupervised learning. 

*Supervised learning* is a machine learning technique that is used to train/teach a machine using **labeled** data. Labeled data implies that the data is already tagged with the correct answer. Teaching the machine on labeled data allows for future data to be correctly predicted.  Under the umbrella of supervised learning, there are 2 categories of algorithms:

**Classification**: Output variable is a category, so we are looking to categorize the output
**Regression**: Output variable is a real value, so we are looking to predict the value of the output

*Unsupervised Learning* is a machine learning technique that uses used to train/teach a machine using that isn’t labeled or classified. The machine is now responsible for grouping the data according to similarities in characteristics without prior training. This is much harder because there is no “teacher” here, meaning the machine is tasked with finding the hidden structure in the unlabeled data.  Under the umbrella of unsupervised  learning, there are 2 categories of algorithms:

**Clustering**: Grouping of cata to find similarities 
**Association**: Discover riles that describe large portions of data

|| Supervised Learning  | Unsupervised Learning |
|--------|--------|--|
| Input | Labeled Data  | Unlabeled, uncategorized |
| Accuracy | Highly accurate   |Less acurate |
| Output | Categorized or real values | Groupings|

: Comparison of Supervised and Unsupervised Learning {tbl-colwidths="[25,25]"}

# What is Clustering?
As mentioned, clustering is a type of unsupervised learning. Here, we are grouping data so that each group, or cluster, exhibits similar qualities. Ultimately, the goal of clustering to uncover intrinsic patterns and structures within data that can be used for analysis. 

# How Does Clustering Work?
Algorithms that are focused on clustering measure similarity between data points across a set of features. Features should be continuous variables but can be categorical. However, categorical data needs special encoding. Data points in the cluster that appear close to each other based on the features are grouped together, which the data points that are far away are separated into different clusters. There are several approaches to clustering, including: 

* **K-Means**: Grouping of data points in K clusters by minimizing the intra-cluster sum-of-squares. This requires setting the number of clusters up front, as we’ll see in the application section.
* **Hierarchial**: Hierarchy of clusters are built iteratively
* **DBSCAN**: groups dense regions of points and considers the sparse areas as outliers. Intuitively detects arbitrary cluster shapes. 
* **Gaussian Mixture Models**: Fits data as a mixture of Gaussian distributions where clusters are modeled using mean and covariance parameters. 

# Clustering Use Cases
So when is it a good time to use clustering? As mentioned, the best time to use clustering would be when we have data that is unlabeled. The following are more specific reasons to use clustering

1. **Exploratory Data Analysis**: Clustering can help to reveal intrinsic groups and patterns in data without prior knowledge. It can open the door for further analysis by uncovering segements that were previously unknown
2. **Customer Segementation**: Cluster customers based on certain attributes like demographics, purchasing beahvior, and more to achieve targeted marketing
3. **Social Network Analysis**: Identify communities within a social network by clustering nodes based on connectivity and usage patterns
4. **Anomaly Detection**: Detect anomalous data points that might not fit into a cluster to detect potential fraud or network attacks.

# K-Means Clustering
K-Means is one of the most popular clustering algorithms that is used for discovering intrinsic groups in unlabeled data. 
## How Does K-Means Work?
K-Means groups data points into a predefined number of clusters, labeled as $k$. It does this by minimizing the intra-cluster sum-of-squares. The general process is as follows: 

1. Initialize $k$ random centroids for the clusters 
2. Assign each data point to its closest centroid point based on Euclidean distance 
3. Recompute the centroids as the mean of all data points assigned to that cluster
4. Repeat steps 2-3 until the centroids converge or the maximum number of iterations is achieved. 

## Example of K-Means Clustering
We can actually apply K-Means clustering in Python. Similar to the last post, I want to demonstrate how that can be done in code using a dataset of Pokemon from Generations 1-7.

First we can start off by importing the libraries that are needed for the code.

```{python}
from sklearn.cluster import KMeans
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
```

We are then going to want to load our data. Again, it's from the Pokemon dataset.

```{python}
data = pd.read_csv("../../datasets/pokemon.csv", sep=",")
data
```

Recall that clustering requires features to be **continuous variables**. In this dataset, we have a couple of different continuous variables, such as:

* `attack`
* `defense`
* `base_total`
* `sp_attack`
* `sp_defense`
* `height_m`
* `percentage_male`
* `speed`
* `weight_kg`

Some of these variables are kind of useless though because they aren't going to give us anything interesting. Instead we want to focus on the stats associated with each Pokemon. More specifically, we want to focus on 

* `attack`
* `defense`
* `sp_attack`
* `sp_defense`

Honestly, I had trouble trying to decide on whether I wanted to cluster based on `attack` and `defense` or `sp_attack` and `sp_defense`. As a result, I ended up combining them in a way so that `attack` and `sp_attack` add up to `total_attack` and `defense` and `sp_defense` add up to `total_defense`. 

```{python}
data['Total Attack'] = data['attack'] + data['sp_attack']
data['Total Defense'] = data['defense'] + data['sp_defense']
```

Here we are storing data for `total_attack` and `total_defense` into variable `X`

```{python}
x_val = 'Total Attack'
y_val = 'Total Defense'
values = [x_val, y_val]
X = data[values] 
```

After we have our data, we need to determine the number of clusters, or $k$. This is up to you, but I wanted to cluster it by 3 so we can have clusters of **weak**, **intermediate**, and **strong**. 

```{python}
kmeans = KMeans(n_clusters=3)
```

We then need to compute the K-Means clusters by using the `fit()` function. Here we supply the method with the data that was gathered earlier.
```{python}
kmeans.fit(X)
```

We can then store the labels of the cluster into `labels` so that it can be used for applying colors to each cluster with the `colormap` array
```{python}
labels = kmeans.labels_
colormap = np.array(['red', 'green', 'blue'])
```

Afterwards, we are going to actually plot a scatter graph of the clustered data, using `total_attack` and `total_defense` as features

```{python}
plt.scatter(X['Total Attack'], X['Total Defense'], c=colormap[labels])
plt.title('Pokemon Attack and Defense Clusters')
plt.xlabel('Total Attack')
plt.ylabel('Total Defense')

plt.show()
```

In the above clustering, data points labeled in red are considered "weak", green are considered "intermediate", and blue is considered "strong.

I'm going to be replaying Pokemon games over the winter break so I'm hoping this scatter will give me an indication as to what Pokemon I should catch!

# Gaussian Mixture Models (GMMs)
Gausian Mixture Models (GMMs) are probabilistic models that assumes data points are generated from a mixture of Gaussian distributions. GMMs are a popular clustering algorithm with applications such as density estimation, data compression, and more. 

## How Does GMMs Work
A GMM models each cluster as a Gaussian distribuion, characterized by a mean and covariance matrix. The complete model is a weighted sum of the component Gaussian densities.

The GMM fitting process determines the parameters of each Gaussian as well as the mixture weights. This is done through iterative expectation-maximiation algorithm that converges to find the maximum likelihood estimates.

The number of Gaussian components corresponds to the number of clusters. Choosing the right number of clusters is important to avoid underfitting or overfitting.

## Example of GMM
Here we are importing a couple of different libraries that have been used before. We are importing `GaussianMixture` in order to create Gaussian Mixture Models.

```{python}
import pandas as pd
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt
```

The dataset being imported consists of customer data, including information such as 

* `Gender`
* `Age`
* `Annual Income ($)`
* `Spending Score (1-100)`
* `Profession`
* `Work Experience`
* `Family Size`

```{python}
customer_data = pd.read_csv('../../datasets/Customers.csv')
```

I am particularly interested in using variables that are continuous, which are `Age`, `Annual Income`, and `Work Experience`. We are using these variables as features for segmentation

```{python}
selected_features = ['Age', 'Annual Income ($)', 'Work Experience']
```

Here we are preprocessing by extracting and scaling the selected features

```{python}
customer_features = customer_data[selected_features]
customer_features = (customer_features - customer_features.mean()) / customer_features.std()
```

We can create a GMM by creating 3 clusters of the data and then fitting it based on the preprocessed data. 

We are then predicting the labels and then assigning it to a column called `Cluster`

```{python}
num_clusters = 3
gmm = GaussianMixture(n_components=num_clusters)
gmm.fit(customer_features)
cluster_labels = gmm.predict(customer_features)
customer_data['Cluster'] = cluster_labels
```

Finally, we plot the data on a scatterplot.

```{python}
plt.scatter(x=customer_data["Age"], y=customer_data["Annual Income ($)"], c=cluster_labels, cmap="Set1")
plt.xlabel("Age")
plt.ylabel("Income")
plt.suptitle('Customer Segmentation using GMM')
plt.show()
```